{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5LXj7efE5ovRTkSmJzrPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0459ac5b851404ea2ecbd64b3af99e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4b2a5312af543dab6d1e38e91d53017",
              "IPY_MODEL_23d280fc6d894d0b9eac78bfbdd1e241",
              "IPY_MODEL_474de28dda874915aaeed007d869c129"
            ],
            "layout": "IPY_MODEL_42b11a7b2b3e4d468e90f8b8f3418c90"
          }
        },
        "c4b2a5312af543dab6d1e38e91d53017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d1bd8c45fb240729d1b8e590d038553",
            "placeholder": "​",
            "style": "IPY_MODEL_69928a3bcba64213992562fde5defcf9",
            "value": "model.safetensors: 100%"
          }
        },
        "23d280fc6d894d0b9eac78bfbdd1e241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aa20e5eb53e459cb0ddd1be727188f4",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b32ce2473f3f46fa852a662bbaa2d798",
            "value": 440449768
          }
        },
        "474de28dda874915aaeed007d869c129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1061015c897342e3939053c2da5554bc",
            "placeholder": "​",
            "style": "IPY_MODEL_f54f7972025e4502ade413c87651d2c1",
            "value": " 440M/440M [00:05&lt;00:00, 69.4MB/s]"
          }
        },
        "42b11a7b2b3e4d468e90f8b8f3418c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d1bd8c45fb240729d1b8e590d038553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69928a3bcba64213992562fde5defcf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9aa20e5eb53e459cb0ddd1be727188f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32ce2473f3f46fa852a662bbaa2d798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1061015c897342e3939053c2da5554bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f54f7972025e4502ade413c87651d2c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soutrik71/MInMaxBERT/blob/main/notebook/BERTEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Objective of this notebook is to experiment with the tokenization provided by bert model"
      ],
      "metadata": {
        "id": "t7BjTTmaJ423"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Word Embeddings"
      ],
      "metadata": {
        "id": "K_OuNuBN2MVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tutorial demonstrates how to leverage BERT for extracting word and sentence embeddings from text data. These embeddings offer:\n",
        "\n",
        "Enhanced keyword/search expansion, semantic search, and information retrieval capabilities. By accurately capturing contextual meaning, they enable precise matching of customer queries with relevant content. For instance, even if there's no direct overlap in keywords or phrases, BERT embeddings facilitate retrieving well-documented searches or answers to customer questions.\n",
        "\n",
        "Advanced feature inputs for downstream NLP models, such as LSTMs or CNNs. Unlike traditional approaches like Word2Vec, where word embeddings remain static regardless of context, BERT dynamically adjusts word representations based on surrounding words. This context-awareness leads to more accurate feature representations and improved model performance. For example, consider the sentences: \"The man was accused of robbing a bank\" and \"The man went fishing by the bank of the river.\" While Word2Vec would assign the same embedding to \"bank\" in both sentences, BERT generates distinct embeddings considering the contextual nuances."
      ],
      "metadata": {
        "id": "4hUUSeJZQx1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AJWFMylQx5w",
        "outputId": "17b79013-be22-4947-c066-c5d965495f91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XbTJ3GFZQx9Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"]=\"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\""
      ],
      "metadata": {
        "id": "jsfvrASswvxu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained model tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "RVdxCJn-QyBB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare our input data for BERT, we need to adhere to specific formatting requirements:\n",
        "\n",
        "* Special Tokens: We require special tokens like [SEP] to indicate the end of a sentence or the separation between two sentences, and [CLS] at the beginning of our text, which BERT expects regardless of the task.\n",
        "\n",
        "* Tokens from BERT's Vocabulary: Our input tokens should be chosen from the fixed vocabulary used by BERT.\n",
        "\n",
        "* Token IDs: We need to convert our tokens into their corresponding token IDs using BERT's tokenizer.\n",
        "\n",
        "* Mask IDs: We use mask IDs to distinguish between actual tokens and padding elements in the sequence.\n",
        "\n",
        "* Segment IDs: These are used to differentiate between different sentences in the input sequence.\n",
        "\n",
        "* Positional Embeddings: These embeddings indicate the position of each token within the sequence.\n",
        "\n",
        "Although the transformers interface offers a convenient way to handle these requirements through functions like tokenizer.encode_plus, we'll perform most of these steps manually for the sake of introducing working with BERT."
      ],
      "metadata": {
        "id": "Y87DZbgeQyD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT can take as input either one or two sentences, and uses the special token [SEP] to differentiate them. The [CLS] token always appears at the start of the text, and is specific to classification tasks."
      ],
      "metadata": {
        "id": "ct14RNYfQyHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\""
      ],
      "metadata": {
        "id": "u7gLeaNrQyLQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3F_x-6DQyOm",
        "outputId": "db5e7362-0b3f-4277-8e4d-604571b02afd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordPiece Model\n",
        "\n",
        "The appearance of smaller subwords and characters with hash signs (##) in front of them is a result of the BERT tokenizer's WordPiece model. Here's why it looks this way:\n",
        "\n",
        "WordPiece Model: BERT's tokenizer operates using a WordPiece model, which constructs a vocabulary comprising individual characters, subwords, and words that best represent the language data it was trained on.\n",
        "\n",
        "Vocabulary Size Limit: The BERT tokenizer's vocabulary size is limited to 30,000 tokens. The WordPiece model selects the most common English words, subwords, and characters to populate this vocabulary.\n",
        "\n",
        "Composition of Vocabulary:\n",
        "\n",
        "Whole words\n",
        "Subwords occurring at the beginning of a word or independently (e.g., \"em\" in \"embeddings\" shares the same vector as the standalone \"em\" in \"go get em\").\n",
        "Subwords not at the beginning of a word, denoted by preceding '##'.\n",
        "Individual characters\n",
        "Tokenization Process: When tokenizing a word, the tokenizer first checks if the entire word exists in the vocabulary. If not, it attempts to decompose the word into the largest possible subwords found in the vocabulary. As a last resort, it decomposes the word into individual characters.\n",
        "\n",
        "Handling Out of Vocabulary Words: Instead of assigning out-of-vocabulary words to a generic unknown token, BERT decomposes them into subword and character tokens. This approach helps retain some contextual meaning of the original word.\n",
        "\n",
        "Representation of Out-of-Vocabulary Words: For instance, the word \"embeddings\" would be split into subword tokens like ['em', '##bed', '##ding', '##s']. By retaining these subword embeddings, it's possible to approximate the vector for the original word, and even generate an average vector from these subword embeddings.\n",
        "\n",
        "Overall, this tokenization strategy allows BERT to handle out-of-vocabulary words more effectively, preserving some contextual information rather than treating them as entirely unknown entities."
      ],
      "metadata": {
        "id": "nbOTtNmTQyR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.vocab.keys())[:10], list(tokenizer.vocab.keys())[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV0vjxggQyVi",
        "outputId": "7b660a4c-e102-4307-e4a6-c3095d981636"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['[PAD]',\n",
              "  '[unused0]',\n",
              "  '[unused1]',\n",
              "  '[unused2]',\n",
              "  '[unused3]',\n",
              "  '[unused4]',\n",
              "  '[unused5]',\n",
              "  '[unused6]',\n",
              "  '[unused7]',\n",
              "  '[unused8]'],\n",
              " ['##！', '##（', '##）', '##，', '##－', '##．', '##／', '##：', '##？', '##～'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After breaking the text into tokens, we then have to convert the sentence from a list of strings to a list of vocabulary indeces.\n",
        "101 marks starting cls and 102 marks the ending"
      ],
      "metadata": {
        "id": "IMtxS8_ly5Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "print(index_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpIV_r6Qy5Xz",
        "outputId": "be3e61f8-fb9a-4bdd-9843-e47457612ff5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2182, 2003, 1996, 6251, 1045, 2215, 7861, 8270, 4667, 2015, 2005, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEGMENT ID:\n",
        "\n",
        "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
        "\n",
        "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1.\n",
        "\n",
        "This is same as mask id s that we use in our classification model"
      ],
      "metadata": {
        "id": "VefHI_tMy5bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segments_ids = [1] * len(tokenized_text)\n",
        "print(segments_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmLwuVa2y5e3",
        "outputId": "e828f904-9b30-44a9-d07a-96d1f3bdd90f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Embeddings\n",
        "\n",
        "Extract the right embeddings from the model"
      ],
      "metadata": {
        "id": "J7JXYiEDQyYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([index_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "metadata": {
        "id": "X6M_ci-kQylk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797,
          "referenced_widgets": [
            "b0459ac5b851404ea2ecbd64b3af99e8",
            "c4b2a5312af543dab6d1e38e91d53017",
            "23d280fc6d894d0b9eac78bfbdd1e241",
            "474de28dda874915aaeed007d869c129",
            "42b11a7b2b3e4d468e90f8b8f3418c90",
            "5d1bd8c45fb240729d1b8e590d038553",
            "69928a3bcba64213992562fde5defcf9",
            "9aa20e5eb53e459cb0ddd1be727188f4",
            "b32ce2473f3f46fa852a662bbaa2d798",
            "1061015c897342e3939053c2da5554bc",
            "f54f7972025e4502ade413c87651d2c1"
          ]
        },
        "id": "A19X9jdR02cx",
        "outputId": "99c8e705-d016-4648-b58f-50587f052c84"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0459ac5b851404ea2ecbd64b3af99e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the text through BERT, and collect all of the hidden states produced\n",
        "# from all 12 layers.\n",
        "with torch.no_grad():\n",
        "  outputs = model(tokens_tensor, segments_tensors)\n"
      ],
      "metadata": {
        "id": "-Ud7KYc602gd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YmYh9pN02jj",
        "outputId": "420e5db9-2681-4ae6-e3d0-a084517a5244"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states = outputs[\"hidden_states\"]"
      ],
      "metadata": {
        "id": "YAn9md3R02oO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hidden_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ2BN7Ns8NB9",
        "outputId": "8974acc9-80cf-432a-ae47-dfaf8e09e9cb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs['last_hidden_state'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F33izgIE8NTy",
        "outputId": "fdb9299a-8f17-4c5b-d228-0ca8f1a78a01"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 14, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The full set of hidden states for this model, stored in the object hidden_states, is a little dizzying. This object has four dimensions, in the following order:\n",
        "\n",
        "The layer number (13 layers)\n",
        "The batch number (1 sentence)\n",
        "The word / token number (22 tokens in our sentence)\n",
        "The hidden unit / feature number (768 features)\n",
        "Wait, 13 layers? Doesn’t BERT only have 12? It’s 13 because the first element is the input embeddings, the rest is the outputs of each of BERT’s 12 layer"
      ],
      "metadata": {
        "id": "AeNhtVfr8NXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "layer_i = 0\n",
        "\n",
        "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
        "\n",
        "print(f\"length of the actual input token is {len(index_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VFB8Yjw8Nbp",
        "outputId": "aa417c86-f109-49d0-8d6c-5b25d0d293d8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
            "Number of batches: 1\n",
            "Number of tokens: 14\n",
            "Number of hidden units: 768\n",
            "length of the actual input token is 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so a batch of 14 tokens has been converted into embeddings of 768 abstrct features for each\n",
        "\n",
        "Current dimensions:\n",
        "\n",
        "[# layers, # batches, # tokens, # features]\n",
        "\n",
        "Desired dimensions:\n",
        "\n",
        "[# tokens, # layers, # features]"
      ],
      "metadata": {
        "id": "USe_sz4q8Nfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each layer in the list is a torch tensor. First one as it represents the op of embedding layer which is fed to next 12 transformer layers\n",
        "print('Tensor shape for each layer: ', hidden_states[0].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBLr1UBG8Njx",
        "outputId": "2cbd367a-aec5-40c6-e6ff-4583daf6f6fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor shape for each layer:  torch.Size([1, 14, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "token_embeddings.size() # 13 layers of concatenated representation of the sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiGyWxBM8Nnj",
        "outputId": "ff5ff0b2-2038-4efd-e7fa-9bfcaa2dc0b8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 1, 14, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove dimension 1, the \"batches\" as it os not needed\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZmJkAav8Nq7",
        "outputId": "24aaf60d-613e-4f6a-8a06-408456ae5770"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 14, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now what we want is representation of each token across all the layers\n",
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYba6on98Nt_",
        "outputId": "32aedef7-4757-48d8-f195-0ed770587637"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14, 13, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, what do we do with these hidden states? We would like to get individual vectors for each of our tokens, or perhaps a single vector representation of the whole sentence, but for each token of our input we have 13 separate vectors each of length 768.\n",
        "\n",
        "In order to get the individual vectors we will need to combine some of the layer vectors…but which layer or combination of layers provides the best representation?"
      ],
      "metadata": {
        "id": "fLpfosz88Nxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Vectors\n",
        "\n",
        "Create appropriate vector representation for each word:\n",
        "\n",
        "concat/sum"
      ],
      "metadata": {
        "id": "DUjDe31j8N0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores the token vectors\n",
        "token_vecs_cat = []\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    print(token.shape)\n",
        "    # Concatenate the vectors (that is, append them together) from the last\n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072 for last 4\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    # alt :: token[-4:].reshape(1,-1).squeeze()\n",
        "    print(cat_vec.shape)\n",
        "\n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKbme3fZ_AuA",
        "outputId": "88a12fa1-2ab0-4c12-968c-de08fb437d86"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "torch.Size([13, 768])\n",
            "torch.Size([3072])\n",
            "Shape is: 14 x 3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for each token we have a representation of 3072 elements\n",
        "len(token_vecs_cat) , token_vecs_cat[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "679afoJF_AxP",
        "outputId": "f00eb0ed-0501-4f16-bfc1-320c9e4c95b6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, torch.Size([3072]))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores the token vectors\n",
        "token_vecs_sum = []\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "  # Sum the vectors from the last four layers.\n",
        "  sum_vec = torch.sum(token[-4:], dim=0)\n",
        "  # Use `sum_vec` to represent `token`.\n",
        "  token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBiOeL1s_A0r",
        "outputId": "6a241f31-2c68-45c0-8d09-b746d544fde2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape is: 14 x 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Vectors\n",
        "\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the second to last hiden layer of each token producing a single 768 length vector."
      ],
      "metadata": {
        "id": "2abDJgqCBjxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "# !gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
      ],
      "metadata": {
        "id": "22n-JSIiBj1F"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# `hidden_states` has shape [13 x 1 x 14 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [14 x 768]\n",
        "token_vecs = hidden_states[-2][0]\n",
        "\n",
        "# Calculate the average of all 14 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)"
      ],
      "metadata": {
        "id": "xTB2O2QwBj4-"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNu42TfOBEZF",
        "outputId": "1d62f20f-7d0d-48b3-8fb0-a21eeef8fd6c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pooling Strategy & Layer Choice\n",
        "\n",
        "While concatenation of the last four layers produced the best results on this specific task, many of the other methods come in a close second and in general it is advisable to test different versions for your specific application: results may vary.\n",
        "\n",
        "This is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information.\n",
        "\n",
        "http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png"
      ],
      "metadata": {
        "id": "bc_FWh93BNh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
        "https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX#scrollTo=E_t4cM6KLc98\n",
        "https://colab.research.google.com/drive/1fCKIBJ6fgWQ-f6UKs7wDTpNTL9N-Cq9X"
      ],
      "metadata": {
        "id": "XVLWGuXN_A7F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKQ68gTA8N3p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}